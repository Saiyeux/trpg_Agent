"""
AIæœåŠ¡é…ç½®å’Œæ£€æµ‹

è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„æœ¬åœ°AIæœåŠ¡ï¼Œæä¾›é…ç½®ç®¡ç†ã€‚
"""

import requests
import json
import time
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from Agent.client.model_client import APIType


@dataclass
class AIServiceConfig:
    """AIæœåŠ¡é…ç½®"""
    name: str
    api_type: APIType
    base_url: str
    model_name: str
    context_limit: int
    available: bool = False
    response_time: float = 0.0
    error_message: str = ""


class AIServiceDetector:
    """AIæœåŠ¡æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.services: List[AIServiceConfig] = []
        self.preferred_service: Optional[AIServiceConfig] = None
    
    def detect_services(self) -> List[AIServiceConfig]:
        """æ£€æµ‹æ‰€æœ‰å¯ç”¨çš„AIæœåŠ¡"""
        self.services = []
        
        # æ£€æµ‹Ollama
        ollama_configs = [
            ("Ollama-qwen2.5", APIType.OLLAMA, "http://localhost:11434/api/generate", "qwen2.5:3b", 32000),
            ("Ollama-llama3.2", APIType.OLLAMA, "http://localhost:11434/api/generate", "llama3.2:3b", 32000),
            ("Ollama-gemma2", APIType.OLLAMA, "http://localhost:11434/api/generate", "gemma2:2b", 32000),
        ]
        
        for name, api_type, url, model, context_limit in ollama_configs:
            config = AIServiceConfig(name, api_type, url, model, context_limit)
            if self._test_ollama_service(config):
                self.services.append(config)
        
        # æ£€æµ‹LM Studio
        lm_studio_configs = [
            ("LM-Studio", APIType.LM_STUDIO, "http://localhost:1234/v1/chat/completions", "auto", 128000),
        ]
        
        for name, api_type, url, model, context_limit in lm_studio_configs:
            config = AIServiceConfig(name, api_type, url, model, context_limit)
            if self._test_lm_studio_service(config):
                self.services.append(config)
        
        # é€‰æ‹©é¦–é€‰æœåŠ¡
        if self.services:
            # ä¼˜å…ˆé€‰æ‹©å“åº”æ—¶é—´æœ€å¿«çš„æœåŠ¡
            self.preferred_service = min(self.services, key=lambda s: s.response_time)
        
        return self.services
    
    def _test_ollama_service(self, config: AIServiceConfig) -> bool:
        """æµ‹è¯•OllamaæœåŠ¡"""
        try:
            start_time = time.time()
            
            # é¦–å…ˆæ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
            list_url = config.base_url.replace('/api/generate', '/api/tags')
            response = requests.get(list_url, timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [model['name'] for model in models]
                if config.model_name not in model_names:
                    config.error_message = f"æ¨¡å‹ {config.model_name} ä¸å­˜åœ¨ã€‚å¯ç”¨æ¨¡å‹: {', '.join(model_names)}"
                    return False
            
            # æµ‹è¯•ç”Ÿæˆè¯·æ±‚
            test_payload = {
                'model': config.model_name,
                'prompt': 'ä½ å¥½',
                'stream': False,
                'options': {'num_predict': 10}
            }
            
            response = requests.post(config.base_url, json=test_payload, timeout=10)
            response.raise_for_status()
            
            result = response.json()
            if 'response' in result:
                config.available = True
                config.response_time = time.time() - start_time
                return True
            else:
                config.error_message = "å“åº”æ ¼å¼é”™è¯¯"
                return False
                
        except requests.exceptions.ConnectionError:
            config.error_message = "è¿æ¥å¤±è´¥ - è¯·ç¡®è®¤Ollamaæ­£åœ¨è¿è¡Œ"
            return False
        except requests.exceptions.Timeout:
            config.error_message = "è¯·æ±‚è¶…æ—¶"
            return False
        except Exception as e:
            config.error_message = f"æµ‹è¯•å¤±è´¥: {str(e)}"
            return False
    
    def _test_lm_studio_service(self, config: AIServiceConfig) -> bool:
        """æµ‹è¯•LM StudioæœåŠ¡"""
        try:
            start_time = time.time()
            
            test_payload = {
                'model': config.model_name,
                'messages': [{'role': 'user', 'content': 'ä½ å¥½'}],
                'max_tokens': 10,
                'stream': False
            }
            
            response = requests.post(config.base_url, json=test_payload, timeout=10)
            response.raise_for_status()
            
            result = response.json()
            if 'choices' in result and len(result['choices']) > 0:
                config.available = True
                config.response_time = time.time() - start_time
                return True
            else:
                config.error_message = "å“åº”æ ¼å¼é”™è¯¯"
                return False
                
        except requests.exceptions.ConnectionError:
            config.error_message = "è¿æ¥å¤±è´¥ - è¯·ç¡®è®¤LM Studioæ­£åœ¨è¿è¡Œä¸”å·²åŠ è½½æ¨¡å‹"
            return False
        except requests.exceptions.Timeout:
            config.error_message = "è¯·æ±‚è¶…æ—¶"
            return False
        except Exception as e:
            config.error_message = f"æµ‹è¯•å¤±è´¥: {str(e)}"
            return False
    
    def get_preferred_service(self) -> Optional[AIServiceConfig]:
        """è·å–é¦–é€‰AIæœåŠ¡"""
        return self.preferred_service
    
    def get_available_services(self) -> List[AIServiceConfig]:
        """è·å–æ‰€æœ‰å¯ç”¨æœåŠ¡"""
        return [s for s in self.services if s.available]
    
    def print_service_status(self):
        """æ‰“å°æœåŠ¡çŠ¶æ€"""
        print("=" * 60)
        print("AIæœåŠ¡æ£€æµ‹ç»“æœ")
        print("=" * 60)
        
        if not self.services:
            print("âŒ æœªæ£€æµ‹åˆ°ä»»ä½•AIæœåŠ¡")
            self._print_setup_instructions()
            return
        
        available_count = len(self.get_available_services())
        print(f"æ£€æµ‹åˆ° {len(self.services)} ä¸ªæœåŠ¡ï¼Œå…¶ä¸­ {available_count} ä¸ªå¯ç”¨\n")
        
        for service in self.services:
            status = "âœ… å¯ç”¨" if service.available else "âŒ ä¸å¯ç”¨"
            print(f"{status} {service.name}")
            print(f"   æ¨¡å‹: {service.model_name}")
            print(f"   åœ°å€: {service.base_url}")
            
            if service.available:
                print(f"   å“åº”æ—¶é—´: {service.response_time:.2f}s")
                print(f"   ä¸Šä¸‹æ–‡é™åˆ¶: {service.context_limit}")
            else:
                print(f"   é”™è¯¯: {service.error_message}")
            print()
        
        if self.preferred_service:
            print(f"ğŸŒŸ æ¨èæœåŠ¡: {self.preferred_service.name}")
        else:
            print("âš ï¸  æ²¡æœ‰å¯ç”¨çš„AIæœåŠ¡")
            self._print_setup_instructions()
    
    def _print_setup_instructions(self):
        """æ‰“å°è®¾ç½®è¯´æ˜"""
        print("\n" + "=" * 60)
        print("AIæœåŠ¡è®¾ç½®è¯´æ˜")
        print("=" * 60)
        print("""
1. Ollama è®¾ç½®:
   å®‰è£…: curl -fsSL https://ollama.ai/install.sh | sh
   å¯åŠ¨: ollama serve
   ä¸‹è½½æ¨¡å‹: ollama pull qwen2.5:3b
   
2. LM Studio è®¾ç½®:
   ä¸‹è½½: https://lmstudio.ai/
   å¯åŠ¨ååœ¨æœ¬åœ°æœåŠ¡å™¨æ ‡ç­¾é¡µå¯åŠ¨æœåŠ¡
   ç¡®ä¿ç«¯å£è®¾ç½®ä¸º 1234
   
3. æµ‹è¯•è¿æ¥:
   python3 -c "from Agent.config.ai_config import AIServiceDetector; AIServiceDetector().detect_services()"
""")


def detect_and_configure_ai() -> Optional[AIServiceConfig]:
    """æ£€æµ‹å¹¶é…ç½®AIæœåŠ¡"""
    detector = AIServiceDetector()
    detector.detect_services()
    detector.print_service_status()
    return detector.get_preferred_service()


def create_ai_client_from_config(config: AIServiceConfig):
    """ä»é…ç½®åˆ›å»ºAIå®¢æˆ·ç«¯"""
    from Agent.client.model_client import ModelClient
    
    return ModelClient(
        model_name=config.model_name,
        api_type=config.api_type,
        base_url=config.base_url,
        context_limit=config.context_limit
    )


if __name__ == "__main__":
    # å‘½ä»¤è¡Œæµ‹è¯•
    detect_and_configure_ai()