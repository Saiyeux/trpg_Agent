#!/usr/bin/env python3
"""
çœŸå®AIé›†æˆç«¯åˆ°ç«¯æµ‹è¯•

ä½¿ç”¨çœŸå®çš„AIæœåŠ¡ï¼ˆOllama/LM Studioï¼‰è¿›è¡Œå®Œæ•´çš„TRPGæ¸¸æˆæµç¨‹æµ‹è¯•ï¼Œ
åˆ†æAIå“åº”è´¨é‡å’Œæ¸¸æˆä½“éªŒã€‚
"""

import sys
import os
import json
import time
from datetime import datetime
from typing import Dict, Any, List

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from Agent.config.ai_config import detect_and_configure_ai, create_ai_client_from_config
from Agent.implementations.model_bridge import RealModelBridge
from Agent.implementations.execution_engine import RealExecutionEngine
from Agent.implementations.game_state import RealGameState


class AITestAnalyzer:
    """AIæµ‹è¯•åˆ†æå™¨"""
    
    def __init__(self):
        self.test_results = []
        self.start_time = time.time()
    
    def add_test_result(self, test_name: str, user_input: str, ai_response: str, 
                       call_history: List[Dict], execution_history: List[Dict],
                       game_state_before: Dict, game_state_after: Dict):
        """æ·»åŠ æµ‹è¯•ç»“æœ"""
        result = {
            'test_name': test_name,
            'timestamp': time.time(),
            'user_input': user_input,
            'ai_response': ai_response,
            'call_history': call_history,
            'execution_history': execution_history,
            'game_state_before': game_state_before,
            'game_state_after': game_state_after,
            'analysis': self._analyze_response(user_input, ai_response, call_history)
        }
        self.test_results.append(result)
    
    def _analyze_response(self, user_input: str, ai_response: str, call_history: List[Dict]) -> Dict[str, Any]:
        """åˆ†æAIå“åº”è´¨é‡"""
        analysis = {
            'response_length': len(ai_response),
            'has_concrete_results': False,
            'has_vague_language': False,
            'mentions_numbers': False,
            'mentions_damage': False,
            'mentions_hp': False,
            'ai_layers_called': len(call_history),
            'total_response_time': sum(call.get('response_time', 0) for call in call_history),
            'quality_score': 0
        }
        
        # æ£€æŸ¥å…·ä½“ç»“æœ vs æ¨¡ç³Šè¯­è¨€
        concrete_keywords = ['ä¼¤å®³', 'ç‚¹', 'è¡€é‡', 'ç”Ÿå‘½å€¼', 'å‡»ä¸­', 'å¤±è¯¯', 'æˆåŠŸ', 'å¤±è´¥', 'é€ æˆ', 'é™è‡³', 'ä»', 'åˆ°', 'å‘ç°äº†', 'æ²¡æœ‰å‘ç°']
        vague_keywords = ['ä¼¼ä¹', 'å¯èƒ½', 'ä¹Ÿè®¸', 'æ„Ÿå—åˆ°', 'æ°›å›´', 'ä»¿ä½›', 'å¥½åƒ', 'å¤§æ¦‚', 'åº”è¯¥', 'æˆ–è®¸', 'æ„Ÿè§‰', 'çœ‹èµ·æ¥', 'å¬èµ·æ¥']
        
        response_lower = ai_response.lower()
        analysis['has_concrete_results'] = any(keyword in ai_response for keyword in concrete_keywords)
        analysis['has_vague_language'] = any(keyword in ai_response for keyword in vague_keywords)
        analysis['mentions_numbers'] = any(char.isdigit() for char in ai_response)
        analysis['mentions_damage'] = 'ä¼¤å®³' in ai_response
        analysis['mentions_hp'] = any(hp_word in ai_response for hp_word in ['è¡€é‡', 'ç”Ÿå‘½å€¼', 'HP', 'hp'])
        
        # è®¡ç®—è´¨é‡åˆ†æ•° (0-100)
        score = 30  # åŸºç¡€åˆ†
        if analysis['has_concrete_results']: score += 25  # æé«˜å…·ä½“ç»“æœæƒé‡
        if analysis['mentions_numbers']: score += 20    # æé«˜æ•°å€¼æƒé‡
        if analysis['mentions_damage']: score += 10
        if analysis['mentions_hp']: score += 10
        if not analysis['has_vague_language']: score += 25  # å¤§å¹…æé«˜æ— æ¨¡ç³Šè¯­è¨€æƒé‡
        if analysis['ai_layers_called'] == 3: score += 5   # é™ä½æŠ€æœ¯æŒ‡æ ‡æƒé‡
        if analysis['response_length'] > 50: score += 3    # é™ä½é•¿åº¦æƒé‡
        if analysis['total_response_time'] < 10: score += 2  # é™ä½é€Ÿåº¦æƒé‡
        
        analysis['quality_score'] = min(100, score)
        return analysis
    
    def generate_report(self) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        if not self.test_results:
            return "æ²¡æœ‰æµ‹è¯•æ•°æ®"
        
        total_time = time.time() - self.start_time
        avg_quality = sum(result['analysis']['quality_score'] for result in self.test_results) / len(self.test_results)
        
        report = []
        report.append("=" * 80)
        report.append("çœŸå®AIé›†æˆæµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 80)
        report.append(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        report.append(f"æµ‹è¯•æ¡ˆä¾‹: {len(self.test_results)}ä¸ª")
        report.append(f"å¹³å‡è´¨é‡å¾—åˆ†: {avg_quality:.1f}/100")
        report.append("")
        
        # è¯¦ç»†æµ‹è¯•ç»“æœ
        for i, result in enumerate(self.test_results, 1):
            analysis = result['analysis']
            report.append(f"æµ‹è¯• {i}: {result['test_name']}")
            report.append("-" * 40)
            report.append(f"ç”¨æˆ·è¾“å…¥: {result['user_input']}")
            report.append(f"AIå›å¤: {result['ai_response']}")
            report.append(f"è´¨é‡å¾—åˆ†: {analysis['quality_score']}/100")
            report.append(f"AIè°ƒç”¨å±‚æ•°: {analysis['ai_layers_called']}")
            report.append(f"å“åº”æ—¶é—´: {analysis['total_response_time']:.2f}ç§’")
            
            # è´¨é‡åˆ†æ
            quality_items = []
            if analysis['has_concrete_results']: quality_items.append("âœ… åŒ…å«å…·ä½“ç»“æœ")
            if analysis['mentions_numbers']: quality_items.append("âœ… æåŠå…·ä½“æ•°å€¼")
            if analysis['mentions_damage']: quality_items.append("âœ… æè¿°ä¼¤å®³")
            if analysis['mentions_hp']: quality_items.append("âœ… æåŠç”Ÿå‘½å€¼")
            if not analysis['has_vague_language']: quality_items.append("âœ… æ— æ¨¡ç³Šè¡¨è¿°")
            else: quality_items.append("âš ï¸  åŒ…å«æ¨¡ç³Šè¡¨è¿°")
            
            report.append("è´¨é‡æŒ‡æ ‡: " + ", ".join(quality_items))
            report.append("")
        
        # æ€»ç»“å’Œå»ºè®®
        report.append("=" * 40)
        report.append("æµ‹è¯•æ€»ç»“")
        report.append("=" * 40)
        
        if avg_quality >= 80:
            report.append("ğŸ‰ AIå“åº”è´¨é‡ä¼˜ç§€ï¼")
        elif avg_quality >= 60:
            report.append("âœ… AIå“åº”è´¨é‡è‰¯å¥½")
        else:
            report.append("âš ï¸  AIå“åº”è´¨é‡éœ€è¦æ”¹è¿›")
        
        # ç»Ÿè®¡åˆ†æ
        concrete_count = sum(1 for r in self.test_results if r['analysis']['has_concrete_results'])
        vague_count = sum(1 for r in self.test_results if r['analysis']['has_vague_language'])
        
        report.append(f"å…·ä½“ç»“æœç‡: {concrete_count}/{len(self.test_results)} ({concrete_count/len(self.test_results)*100:.1f}%)")
        report.append(f"æ¨¡ç³Šè¡¨è¿°ç‡: {vague_count}/{len(self.test_results)} ({vague_count/len(self.test_results)*100:.1f}%)")
        
        return "\n".join(report)
    
    def save_detailed_results(self, filepath: str):
        """ä¿å­˜è¯¦ç»†æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump({
                'test_metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'total_tests': len(self.test_results),
                    'total_time': time.time() - self.start_time
                },
                'results': self.test_results
            }, f, ensure_ascii=False, indent=2)


def run_real_ai_tests():
    """è¿è¡ŒçœŸå®AIé›†æˆæµ‹è¯•"""
    print("å¼€å§‹çœŸå®AIé›†æˆæµ‹è¯•...")
    
    # 1. æ£€æµ‹AIæœåŠ¡
    print("\n--- æ£€æµ‹AIæœåŠ¡ ---")
    ai_config = detect_and_configure_ai()
    if not ai_config:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„AIæœåŠ¡ï¼Œæµ‹è¯•ç»ˆæ­¢")
        return False
    
    print(f"âœ… ä½¿ç”¨AIæœåŠ¡: {ai_config.name}")
    
    # 2. åˆå§‹åŒ–ç»„ä»¶
    print("\n--- åˆå§‹åŒ–æµ‹è¯•ç»„ä»¶ ---")
    model_client = create_ai_client_from_config(ai_config)
    execution_engine = RealExecutionEngine()
    game_state = RealGameState()
    
    # åˆ›å»ºçœŸå®çš„ModelBridge
    model_bridge = RealModelBridge(
        model_client=model_client,
        execution_engine=execution_engine,
        game_state=game_state
    )
    
    print("âœ… æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    # 3. åˆ›å»ºæµ‹è¯•åˆ†æå™¨
    analyzer = AITestAnalyzer()
    
    # 4. å®šä¹‰æµ‹è¯•åœºæ™¯
    test_scenarios = [
        {
            'name': 'æ”»å‡»å“¥å¸ƒæ—',
            'input': 'æˆ‘æ”»å‡»å“¥å¸ƒæ—',
            'description': 'åŸºç¡€æ”»å‡»æµ‹è¯•ï¼ŒéªŒè¯AIæ˜¯å¦èƒ½ç»™å‡ºå…·ä½“ä¼¤å®³ç»“æœ'
        },
        {
            'name': 'å†æ¬¡æ”»å‡»',
            'input': 'æˆ‘ç»§ç»­æ”»å‡»å“¥å¸ƒæ—',
            'description': 'è¿ç»­æ”»å‡»æµ‹è¯•ï¼ŒéªŒè¯çŠ¶æ€æŒç»­æ€§'
        },
        {
            'name': 'ä½¿ç”¨æ­¦å™¨æ”»å‡»',
            'input': 'æˆ‘ç”¨å‰‘æ”»å‡»å“¥å¸ƒæ—',
            'description': 'æŒ‡å®šæ­¦å™¨æ”»å‡»æµ‹è¯•'
        },
        {
            'name': 'æœç´¢ç¯å¢ƒ',
            'input': 'æˆ‘æœç´¢å‘¨å›´',
            'description': 'æœç´¢åŠŸèƒ½æµ‹è¯•'
        },
        {
            'name': 'æ£€æŸ¥çŠ¶æ€',
            'input': 'æŸ¥çœ‹æˆ‘çš„çŠ¶æ€',
            'description': 'çŠ¶æ€æŸ¥è¯¢æµ‹è¯•ï¼ˆç›®å‰å¯èƒ½ä¸æ”¯æŒï¼‰'
        }
    ]
    
    # 5. æ‰§è¡Œæµ‹è¯•
    print(f"\n--- æ‰§è¡Œ {len(test_scenarios)} ä¸ªæµ‹è¯•åœºæ™¯ ---")
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"\næµ‹è¯• {i}/{len(test_scenarios)}: {scenario['name']}")
        print(f"è¾“å…¥: {scenario['input']}")
        
        # è®°å½•æµ‹è¯•å‰çŠ¶æ€
        state_before = game_state.to_dict()
        
        # æ‰§è¡Œæµ‹è¯•
        start_time = time.time()
        response = model_bridge.process_user_input(scenario['input'])
        end_time = time.time()
        
        # è®°å½•æµ‹è¯•åçŠ¶æ€
        state_after = game_state.to_dict()
        
        print(f"å›å¤: {response}")
        print(f"è€—æ—¶: {end_time - start_time:.2f}ç§’")
        
        # è·å–è°ƒç”¨å†å²
        call_history = model_bridge.get_call_history()
        execution_history = execution_engine.get_execution_history()
        
        # æ·»åŠ å“åº”æ—¶é—´åˆ°è°ƒç”¨å†å²
        for call in call_history:
            if 'timestamp' in call:
                call['response_time'] = call.get('response_time', 1.0)
        
        # è®°å½•æµ‹è¯•ç»“æœ
        analyzer.add_test_result(
            test_name=scenario['name'],
            user_input=scenario['input'],
            ai_response=response,
            call_history=call_history.copy(),
            execution_history=execution_history.copy(),
            game_state_before=state_before,
            game_state_after=state_after
        )
        
        # æ¸…ç©ºå†å²ï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ªæµ‹è¯•
        model_bridge.clear_history()
        execution_engine.clear_history()
        
        # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(1)
    
    # 6. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    print("\n--- ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š ---")
    report = analyzer.generate_report()
    print(report)
    
    # 7. ä¿å­˜è¯¦ç»†ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"test_results_{timestamp}.json"
    report_file = f"test_report_{timestamp}.txt"
    
    analyzer.save_detailed_results(results_file)
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    print(f"âœ… æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    return True


if __name__ == "__main__":
    try:
        success = run_real_ai_tests()
        exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        exit(1)
    except Exception as e:
        print(f"\n\næµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        exit(1)